{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3162124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96e8d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Flatten, MaxPooling1D, Input, Concatenate\n",
    "from keras.models import load_model\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84860780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7f583bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction = ['adventure','fiction','mystery' , 'romance', 'science_fiction']\n",
    "nonfiction = ['government','hobbies','learned','news', 'reviews'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7db7ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction_ids = [x for y in fiction for x in brown.fileids(categories=y)]\n",
    "nonfiction_ids = [x for y in nonfiction for x in brown.fileids(categories=y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ab05625",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for index, fileid in enumerate(fiction_ids+nonfiction_ids):\n",
    "    paras = brown.paras(fileids=fileid)\n",
    "    label = 1 if fileid in fiction_ids else 0\n",
    "#     label = 'fiction' if fileid in fiction_ids else 'non_fiction'\n",
    "    for j, p in enumerate(paras):\n",
    "        if len(p) > 4 and len(p) < 7:\n",
    "            text = ''\n",
    "            for sent in p:\n",
    "                text = text + ' '.join(sent) + ' '\n",
    "            text = text.strip().lower()\n",
    "            temp = {}\n",
    "            temp['id'] = f'{fileid}_para_{j}'\n",
    "            temp['para'] = text\n",
    "            temp['label'] = label\n",
    "            data.append(temp)\n",
    "#     print('Finished', index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc2469e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db56011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['para'].to_list()\n",
    "y  = df['label'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "598cf638",
   "metadata": {},
   "outputs": [],
   "source": [
    "emmbed_dict = {}\n",
    "with open('../resources/glove.6B.100d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:],'float32')\n",
    "        emmbed_dict[word]=vector\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7313a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb (vocab_size, words_to_index):\n",
    "    emb_matrix = np.zeros((vocab_size, 100))\n",
    "    for word, index in words_to_index.items():\n",
    "        embedding_vector =emmbed_dict.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            emb_matrix[index, :] = embedding_vector\n",
    "    return emb_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e637b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.4864 - accuracy: 0.8165 - val_loss: 0.2847 - val_accuracy: 0.9073\n",
      "Epoch 2/3\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.1908 - accuracy: 0.9489 - val_loss: 0.1680 - val_accuracy: 0.9448\n",
      "Epoch 3/3\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.0851 - accuracy: 0.9820 - val_loss: 0.1364 - val_accuracy: 0.9448\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.0431 - accuracy: 0.9943\n",
      "Epoch 1/3\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.3358 - accuracy: 0.8448 - val_loss: 0.2187 - val_accuracy: 0.9161\n",
      "Epoch 2/3\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.1222 - accuracy: 0.9659 - val_loss: 0.1976 - val_accuracy: 0.9227\n",
      "Epoch 3/3\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.0437 - accuracy: 0.9943 - val_loss: 0.1444 - val_accuracy: 0.9360\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.0218 - accuracy: 1.0000\n",
      "Epoch 1/3\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.6296 - accuracy: 0.6689 - val_loss: 0.5043 - val_accuracy: 0.8455\n",
      "Epoch 2/3\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.3616 - accuracy: 0.9026 - val_loss: 0.2274 - val_accuracy: 0.9051\n",
      "Epoch 3/3\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.0860 - accuracy: 0.9763 - val_loss: 0.1490 - val_accuracy: 0.9382\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.0220 - accuracy: 0.9991\n",
      "Epoch 1/3\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.3431 - accuracy: 0.8751 - val_loss: 0.2566 - val_accuracy: 0.8852\n",
      "Epoch 2/3\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.1217 - accuracy: 0.9697 - val_loss: 0.1804 - val_accuracy: 0.9294\n",
      "Epoch 3/3\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.0541 - accuracy: 0.9886 - val_loss: 0.1261 - val_accuracy: 0.9558\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.0211 - accuracy: 1.0000\n",
      "Epoch 1/3\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.3569 - accuracy: 0.8619 - val_loss: 0.2173 - val_accuracy: 0.9272\n",
      "Epoch 2/3\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.1212 - accuracy: 0.9726 - val_loss: 0.1791 - val_accuracy: 0.9382\n",
      "Epoch 3/3\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.0449 - accuracy: 0.9972 - val_loss: 0.1709 - val_accuracy: 0.9448\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.0222 - accuracy: 1.0000\n",
      "Epoch 1/3\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.3245 - accuracy: 0.8562 - val_loss: 0.2097 - val_accuracy: 0.9227\n",
      "Epoch 2/3\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.1095 - accuracy: 0.9763 - val_loss: 0.1953 - val_accuracy: 0.9249\n",
      "Epoch 3/3\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.0374 - accuracy: 0.9962 - val_loss: 0.2569 - val_accuracy: 0.8962\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.0251 - accuracy: 0.9991\n",
      "Epoch 1/3\n",
      "106/106 [==============================] - 40s 373ms/step - loss: 0.3141 - accuracy: 0.8751 - val_loss: 0.2135 - val_accuracy: 0.9183\n",
      "Epoch 2/3\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.0999 - accuracy: 0.9763 - val_loss: 0.2049 - val_accuracy: 0.9161\n",
      "Epoch 3/3\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.0365 - accuracy: 0.9953 - val_loss: 0.1812 - val_accuracy: 0.9227\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.0147 - accuracy: 1.0000\n",
      "Epoch 1/3\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.3446 - accuracy: 0.8562 - val_loss: 0.2268 - val_accuracy: 0.9073\n",
      "Epoch 2/3\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.1057 - accuracy: 0.9707 - val_loss: 0.1598 - val_accuracy: 0.9426\n",
      "Epoch 3/3\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.0350 - accuracy: 0.9953 - val_loss: 0.1924 - val_accuracy: 0.9227\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.0184 - accuracy: 1.0000\n",
      "Epoch 1/3\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.3714 - accuracy: 0.8534 - val_loss: 0.2233 - val_accuracy: 0.9205\n",
      "Epoch 2/3\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.1291 - accuracy: 0.9659 - val_loss: 0.1777 - val_accuracy: 0.9338\n",
      "Epoch 3/3\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.0548 - accuracy: 0.9905 - val_loss: 0.1517 - val_accuracy: 0.9404\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0246 - accuracy: 1.0000\n",
      "Epoch 1/3\n",
      "106/106 [==============================] - 4s 30ms/step - loss: 0.3532 - accuracy: 0.8505 - val_loss: 0.1930 - val_accuracy: 0.9360\n",
      "Epoch 2/3\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.1061 - accuracy: 0.9754 - val_loss: 0.1432 - val_accuracy: 0.9558\n",
      "Epoch 3/3\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.0383 - accuracy: 0.9953 - val_loss: 0.1301 - val_accuracy: 0.9536\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.0234 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "train_scores = []\n",
    "test_scores = []\n",
    "vocab_sizes = []\n",
    "reports = []\n",
    "for i in range(10):\n",
    "    # Split train & test\n",
    "    text_train, text_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=i)\n",
    "    # Tokenize and transform to integer index\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(text_train)\n",
    "    X_train = tokenizer.texts_to_sequences(text_train)\n",
    "    X_test = tokenizer.texts_to_sequences(text_test)\n",
    "    words_to_index = tokenizer.word_index\n",
    "    vocab_size = len(words_to_index) + 1  # Adding 1 because of reserved 0 index\n",
    "    vocab_sizes.append(vocab_size)\n",
    "    maxlen = max(len(x) for x in X_train) \n",
    "    # Add pading to ensure all vectors have same dimensionality\n",
    "    X_train = np.asfarray(pad_sequences(X_train, padding='post', maxlen=maxlen))\n",
    "    X_test = np.asfarray(pad_sequences(X_test, padding='post', maxlen=maxlen))\n",
    "    y_train = np.asfarray(y_train)\n",
    "    y_test = np.asfarray(y_test)\n",
    "    weight = emb(vocab_size, words_to_index)\n",
    "    embedding_dim = 100\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen, weights = [weight]))\n",
    "    model.add(layers.Conv1D(100, 3, activation='relu'))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    # Fit model\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        epochs=3,\n",
    "                        verbose=True,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        batch_size=10)\n",
    "    train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=True)\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "    train_scores.append(train_accuracy)\n",
    "    test_scores.append(test_accuracy)\n",
    "    \n",
    "    pred = model.predict(X_test)\n",
    "    X_pred = np.asfarray([1 if x>0.5 else 0 for x in pred])\n",
    "    report = classification_report(y_test, X_pred, output_dict=True)\n",
    "    reports.append(report)\n",
    "#     print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fccc2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99.72563982009888, 0.2945011366921104)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training Accuracy\n",
    "np.mean(train_scores)*100, np.std(train_scores)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b90d4a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93.6644583940506, 0.8083805374216849)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing Accuracy\n",
    "np.mean(test_scores)*100, np.std(test_scores)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dd2758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d702bce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9393975799218485, 0.008094618048528169)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F1 Score for fiction\n",
    "np.mean([x['1.0']['f1-score'] for x in reports]), np.std([x['1.0']['f1-score'] for x in reports])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2381b0d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9334320976468365, 0.008835571417982356)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F1 score for non-fiction\n",
    "np.mean([x['0.0']['f1-score'] for x in reports]), np.std([x['0.0']['f1-score'] for x in reports])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34a9a6bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.0': {'precision': 0.9800995024875622,\n",
       "  'recall': 0.9036697247706422,\n",
       "  'f1-score': 0.9403341288782816,\n",
       "  'support': 218},\n",
       " '1.0': {'precision': 0.9166666666666666,\n",
       "  'recall': 0.9829787234042553,\n",
       "  'f1-score': 0.9486652977412732,\n",
       "  'support': 235},\n",
       " 'accuracy': 0.9448123620309051,\n",
       " 'macro avg': {'precision': 0.9483830845771144,\n",
       "  'recall': 0.9433242240874488,\n",
       "  'f1-score': 0.9444997133097774,\n",
       "  'support': 453},\n",
       " 'weighted avg': {'precision': 0.9471928437283781,\n",
       "  'recall': 0.9448123620309051,\n",
       "  'f1-score': 0.9446560376703412,\n",
       "  'support': 453}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reports[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dcae4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb958a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a738503d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed6f2a82",
   "metadata": {},
   "source": [
    "## For visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0309a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d96d0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorboard\n",
    "tensorboard.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ebc8e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "    # Tokenize and transform to integer index\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(text_train)\n",
    "X_test = tokenizer.texts_to_sequences(text_test)\n",
    "    \n",
    "words_to_index = tokenizer.word_index\n",
    "vocab_size = len(words_to_index) + 1  # Adding 1 because of reserved 0 index\n",
    "# vocab_sizes.append(vocab_size)\n",
    "\n",
    "maxlen = max(len(x) for x in X_train) \n",
    "    \n",
    "    # Add pading to ensure all vectors have same dimensionality\n",
    "X_train = np.asfarray(pad_sequences(X_train, padding='post', maxlen=maxlen))\n",
    "X_test = np.asfarray(pad_sequences(X_test, padding='post', maxlen=maxlen))\n",
    "y_train = np.asfarray(y_train)\n",
    "y_test = np.asfarray(y_test)\n",
    "\n",
    "weight = emb(vocab_size, words_to_index)\n",
    "embedding_dim = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f02dd363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, 292, 100)          1315900   \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 290, 100)          30100     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 100)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                1010      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,347,021\n",
      "Trainable params: 1,347,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen, weights = [weight]))\n",
    "model.add(layers.Conv1D(filters=100, kernel_size=3, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2650f4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "106/106 [==============================] - 4s 29ms/step - loss: 0.3509 - accuracy: 0.8543 - val_loss: 0.2360 - val_accuracy: 0.9117\n",
      "Epoch 2/3\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.1041 - accuracy: 0.9735 - val_loss: 0.1891 - val_accuracy: 0.9382\n",
      "Epoch 3/3\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.0345 - accuracy: 0.9991 - val_loss: 0.1888 - val_accuracy: 0.9316\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import keras\n",
    "# Define the Keras TensorBoard callback.\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "# Fit model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=3,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10,\n",
    "                    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc1a0c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89821ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf6175ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 26020), started 0:30:18 ago. (Use '!kill 26020' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-477a80f690c20afe\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-477a80f690c20afe\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d648359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f11ea36b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5236408c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10958483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32016934",
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc2Vec : you can train your dataset using Doc2Vec and then use the sentence vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9597e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Average of Word2Vec vectors : You can just take the average of all the word vectors in a sentence. This average vector will represent your sentence vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5254a740",
   "metadata": {},
   "outputs": [],
   "source": [
    "Average of Word2Vec vectors with TF-IDF : this is one of the best approach which I will recommend. Just take the word vectors and multiply it with their TF-IDF scores. Just take the average and it will represent your sentence vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa7d4bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d9f005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9d233e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61ab703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df23493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04d94c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab380c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606fd769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174cf4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22a3082b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c629b566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c387df27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4396fb9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4b0a00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def809a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4644ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dd6e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb9eb06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a8f78263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426fee12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c55f8da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf7a766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e1b4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be525e96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
